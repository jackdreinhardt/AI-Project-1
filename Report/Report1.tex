\documentclass[a4paper,10pt]{article}
\usepackage{fullpage}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{graphicx}

\begin{document}
\begin{center}
Project report on\\
\vspace{0.5cm}
{{\Large \sc Artificial Intelligence Playing the Ricochet Robots Board Game}}\\
\vspace{0.5cm} for 02180 Introduction to Artificial Intelligence
\end{center}
\rule{\textwidth}{0.5pt}
\begin{description}
\item\begin{tabular}{rll}
    \textbf{Contributors:}& Jannis Haberhausen &(s186398)\\ & Jack Reinhardt &(s186182)\\ & Kilian Speiser &(s181993)\\ & Jacob Miller &(s186093) \\
\end{tabular}
\end{description}
\rule{\textwidth}{1pt}

\tableofcontents
\thispagestyle{empty}
\newpage
\section{Game Background}
\subsection{Game Rules}
\label{subsec:gameRules}
\textit{Ricochet Robots} is a competitive game first published in Germany in 1999. In the original game four different colored robots are placed on a 16 by 16 board. The objective of the game is to reach a target that is placed somewhere on the board each round. The targets have different colors matching the colors of the four robots. A red target has to be reached by the red robot, a blue target by the blue robot and so on. The challenge of the game comes from the allowed movements of the robots. All robots can move in four directions: up, down, left and right, however once a robot is moved it will only stop when it hits a wall or another robot. The outer edges of the board, the four middle squares as well as additional squares on the board are blocked on up to two sides by walls. In order to reach the target often robots of other colors need to be used as 'walls'.
\begin{figure}[!htb]
\center{\includegraphics[width=6cm]{figures/originalgame.jpg}}
\caption{The original boardgame 'Ricochet Robots'}
\label{fig:originalgame}
\end{figure}
\subsection{Changes in the Implementation}
In the original game the board consists of four quaters printed on both sides that can be put together such that 96 unique board setups are possible. In a later version of the game there are eight quaters resulting in 1536 possible board configurations. For the assignment only one, randomly chosen, board configuration from the original game has been implemented.
\par
In order to run tests on the AIs an additional 6 by 6 board has been implemented. The game rules are exactly the same, only some squares have walls on three sides. That is because of the reduced space on the small board.
% Game rules, in particular if you choose a game not on the list above or if you make
% simplifications to the game. Make this part as short as possible. If you have simplified
% the rules, explain why and explain what it would take to make AI for the full game.

% What kind of game is it? Relevant questions are e.g.: Single- or multi-player? Competitive
% or cooperative? Zero-sum? Is it a turn-taking game? Perfect or imperfect information
% (full or partial observability)? Deterministic or stochastic? And what does this imply for
% the kind of algorithms that apply to the game?


\section{Game Representation}
\label{sec:gameRep}
The representation of the game state only includes the locations of the game pieces - the robots - and the location of the target. The walls on the board are static,
so these are not included in the game state. More specifically, the state is represented by a tuple of robots, and the target. The possible moves, dictated by Section
\ref{sec:gameRules}, can be retrieved from the tuple in conjunction with the static game board. There is perfect observability in the game, so there is no need to represent the game state as a belief state. The state is simple to represent, which makes the AI algorithms run more efficiently and accurately.


\section{State Space and Complexity}
\label{sec:stateSpace}
The state space and branching factor of Ricochet Robots is large, making the task of reaching the target non-trivial for both humans and AI search algorithms.
The board consists of a sixteen-by-sixteen grid with walls placed in predetermined positions.  With four robots that can be moved anywhere on the board (other
than on top of another robot), this leaves a total of $(16*16)(16*16-1)(16*16-2)(16*16-3) = 4,195,023,360$ board configurations for a single wall setup and
target placement.  Depending on the board setup, some states may not be reachable, but the state space still remains large. Each robot can move in any of the
four directions on the board until it reaches a wall or another robot in its path.  Assuming that each robot has an obstruction in one of the four directions
decreases the possible moves for each robot to three, giving the search algorithm a branching factor of $4*3 = 12$.  This assumption will be true in most cases since
the robot must be stopped by an obstruction before moving in a different direction. \\

The large state space and branching factor makes the problem of reaching the goal state difficult for traditional AI search algorithms with limited memory and time.
In order to simplify the game and drastically reduce the size of the state space and the branching factor, we implemented Ricochet Robots in a way that allows the
user to choose the size of the board (16x16 or 6x6) as well as the number of robots. In general, the size of the state space can be approximated by $(w*h)^n$ and
the branching factor can be approximated by $n*3$, where w is the board width, h is the board height and n is the number of robots.  This reduced implementation of
Ricochet Robots allowed us to play and test our AI algorithms without surpassing our memory and time limitations.


\section{Search Algorithms and Results}


  \subsection{Recursive Depth-Limited Search} \label{recursiveDFS}
  Given the large branching factor of Ricochet Robots, a depth-limited search gives the AI player a higher potential to find solutions to the more difficult board
  configurations.  Some experimentation is necessary to find the optimal limit to the depth, but when the AI player is attempting to find a better solution than
  its human competitor, the limit could simply be set to one less than the human move count.  \\

  Specific design choices were made in order to optimize the depth-limited search for Ricochet Robots.  The depth-limited search was implemented recursively and
  as a tree search rather than a graph search.  Not keeping track of past states gives the algorithm much less overhead and allows it to expand nodes more quickly,
  increasing the success rate.  The algorithm will start in the initial state by looping through each possible move for each robot and continue traversing the tree
  recursively until it either reaches a solution or it reaches the specified depth limit.  Once it reaches the depth limit, it will return to the previous level of
  recursion to search the next branch at that depth.  \\

  One negative aspect of the depth-limited AI player is that it usually does not find an optimal solution.  Most solutions contain extraneous moves that do nothing
  towards reaching the goal state.  In order to alleviate this issue, two aspects were added to the algorithm.  First, when checking if a move is possible, the
  algorithm also checks if the move is the opposite of the previous move (i.e if the red robot just moved North, do not allow it to move South).  This cuts off an
  entire repeated branch of the search tree, giving the algorithm more time to search new branches.  Second, once the algorithm finds a solution, it will optimize it
  by removing non-essential moves.  It does this by iteratively deleting moves from the end and testing if the goal state is still reached. \\

  \begin{figure}[h!]
    \centering
   \includegraphics[width=0.45\linewidth]{figures/img1.PNG}
    \includegraphics[width=0.45\linewidth]{figures/img2.PNG}
    \caption{Depth-limited Search Testing Results}
    \label{fig:DFS_chart}
  \end{figure}

  Figure \ref{fig:DFS_chart} shows the testing results for the depth-limited search with different maximum depth limits.  Each algorithm was tested on 30 iterations
  of Ricochet Robots, starting with a random board setup and continuing the next round where the previous round left each of the robots.  Each iteration placed the
  target randomly, meaning that some configurations may be more difficult to find a solution or the optimal solution may be at a larger depth.  The AI player was
  given sixty seconds to find a solution. \\

  For both the simplified implementation (6x6 board and three robots) and the full implementation (16x16 board and four robots), a depth limit of ten was able to
  find a solution most frequently.  Depth limits of six and eight performed considerably worse than that of ten for both tests, implying that many of the solutions were
  past these depth limits.  The AI player with a depth limit of twelve performed poorly on the simplified implementation, but only marginally worse than that of ten in
  the full implementation.  This is likely because the algorithm wastes time searching deep into branches of the search tree when the solutions to the simplified game
  are closer to the initial state.  Overall, the depth-limited AI player would be an adequate match for a human player in both the full and simplified versions of
  Ricochet Robots.\\


  \subsection{Informed Search: Breadth First Search}
  As already mentioned

  The team decided to develop a informed breadth first search to guarantee finding always the goal state by using as few moves as possible. However, this leads as mentioned in \ref{sec:stateSpace} this leads to a tree with up to $16^n$ knots.
  \begin{itemize}
  	\item Difficulty of setting the limit to a certain level
  \end{itemize}


  \subsection{A* Search}
  Two heuristic functions were tested with the A* graph search for their ability to quickly reach the target in Ricochet Robot.
  \begin{enumerate}
    \item Manhattan Distance - let h be equal to the one-norm of the target's location on the board and the location of the robot of corresponding color. \\
    $h = |robots.x - target.x| + |robots.y - target.y|$
    \item Row/Column - for each robot accumulate h according to the following:
    \begin{itemize}
      \item[--] if the robot color matches the target color, then h += 0 if the robot is in the same row or column as the target and h += 1 otherwise
      \item[--] else, h += 0 if the robot is in an adjacent row or column to the target and h += 1 otherwise
    \end{itemize}
  \end{enumerate}
  Neither heuristic function is consistent and therefore this algorithm does not guarantee an optimal solution.  While a breadth-first search is optimal,
  it is increasingly slow at each depth level due to the large branching factor.  The A* search would prioritize certain nodes to expand based on the heuristics
  defined above, hopefully allowing it to reach the target more quickly. \\

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/img3.PNG}
    \includegraphics[width=0.45\linewidth]{figures/img4.PNG}
    \caption{A* Testing Results}
    \label{fig:A*_chart}
  \end{figure}

  Figure \ref{fig:A*_chart} shows the testing results for the A* AI player given the two heuristic functions described above as well as a breadth-first search as a
  baseline (setting h = 0).  Paralleling the tests done with the recursive depth-limited search algorithm, each AI player was tested on 30 random iterations of Ricochet
  Robots with a time limit of sixty seconds. \\

  While BFS performed the best in the simplified implementation, it was beat out by the A* - Manhattan Distance in the full implementation.  BFS will perform best when
  the goal state is at a shallow depth because it is optimal and therefore will never skip over a solution close to the initial state.  This behavior explains BFS's
  success in the simplified game.  Unfortunately, BFS will not be able to search deeper states in the search tree before it runs out of time.  The Manhattan distance
  heuristic is not optimal, which may cause it to miss some shallower solutions that BFS would find, but in the full game the heuristic seems to have helped guide the AI
  player towards a solution quicker than BFS.  The row/column heuristic performed worst in both games and does not seem to be a viable algorithm for Ricochet Robots.
  Due to the higher memory usage and slower run-time per iteration associated with the A* AI player, it cannot search nearly as many states as the recursive depth-limited
  AI player, and therefore had far lower success rates than that of the recursive depth-limited AI player.


  \subsection{Custom Search Algorithm}


\section{Conclusion}



\end{document}
